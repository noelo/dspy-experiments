from ast import List
from typing import Any
import dspy
from dspy.teleprompt import MIPROv2
import pandas as pd
import logging
import sys
import os
from dotenv import load_dotenv
from llama_stack_client import LlamaStackClient
import random
import mlflow

load_dotenv()
LLAMA_STACK_URL = os.getenv("LLAMA_STACK_URL")
REASON_LLM_KEY = os.getenv("REASON_LLM_KEY")
REASON_LLM_HOST = os.getenv("REASON_LLM_HOST")
REASON_LLM_MODEL = os.getenv("REASON_LLM_MODEL")
MLFLOW_HOST = os.getenv("MLFLOW_HOST")


root = logging.getLogger()
root.setLevel(logging.WARN)
handler = logging.StreamHandler(sys.stdout)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
handler.setFormatter(formatter)
root.addHandler(handler)


class AILanguageDetector(dspy.Signature):
    """Determine if the input_text is generated by AI or by a human"""

    input_text: str = dspy.InputField(desc=("text to be evaluated"))
    result: int = dspy.OutputField(desc=("0 for human 1 for AI"))


def score_pred(gold, pred):
    """
    Compute score.
    """
    score = 1.0 if gold == pred.result else 0.0
    return score

def mipro_metric(example, pred, trace=None, pred_name=None, pred_trace=None):
    # Parse gold standard from example
    gold = example["result"]
    # Compute scores for all modules
    score_val = score_pred(gold, pred)
    return score_val


def optimise_prompt_miprov2():
    text_processor = dspy.Predict(AILanguageDetector)
    print(text_processor.dump_state())
    optimizer = MIPROv2(
        metric=mipro_metric,
        auto="light",
        verbose=True
    )
    optimized_program = optimizer.compile(
        text_processor,
        trainset=train_set,
        valset=val_set
    )
    print(optimized_program.dump_state())
    
    optimized_program.save("mipro-prompt.json")

    evaluate_prompt(optimized_program, val_set)



def evaluate_prompt(text_processor: Any, validation_set: List):
    evaluate = dspy.Evaluate(
        devset=validation_set,
        metric=mipro_metric,
        num_threads=5,
        display_table=10,
        display_progress=True,
        provide_traceback=True,
    )
    evaluate(text_processor)


def init_dataset():
    df = pd.read_csv("data/balanced_ai_human_prompts.csv")
    full_dspy_dataset=[]
    for index, row in df.iterrows():
        full_dspy_dataset.append(
            dspy.Example(
                {
                    "input_text": row["text"],
                    "result": row["generated"],
                }
            ).with_inputs("input_text")
        )

    random.Random(0).shuffle(full_dspy_dataset)
    train_set = full_dspy_dataset[: int(len(full_dspy_dataset) * 0.33)]
    val_set = full_dspy_dataset[
        int(len(full_dspy_dataset) * 0.33) : int(len(full_dspy_dataset) * 0.66)
    ]
    test_set = full_dspy_dataset[int(len(full_dspy_dataset) * 0.66) :]

    return full_dspy_dataset, train_set, val_set, test_set


if __name__ == "__main__":
    
    mlflow.set_tracking_uri(MLFLOW_HOST)
    mlflow.set_experiment("AILanguageDetector")
    mlflow.dspy.autolog()
    full_dspy_dataset, train_set, val_set, test_set = init_dataset()
    print(
        f"{len(full_dspy_dataset)}---Training:{len(train_set)}, Validation:{len(val_set)}, Test:{len(test_set)}"
    )

    lls_client = LlamaStackClient(base_url=LLAMA_STACK_URL)
    model_list = lls_client.models.list()
    llm = dspy.LM(
        "openai/" + model_list[0].identifier,
        api_base=LLAMA_STACK_URL + "/v1/openai/v1",
        model_type="chat",
        api_key="this is a fake key",
    )

    dspy.configure(lm=llm)
    dspy.configure_cache(
        enable_disk_cache=False,
        enable_memory_cache=False,
    )
    
    print("Evaluate Base Prompt")
    base_ai_processor = dspy.Predict(AILanguageDetector)
    evaluate_prompt(base_ai_processor,full_dspy_dataset)

    print("Optimize & Save Prompt") 
    optimise_prompt_miprov2()

    print("Evaluate Optimized prompt")
    optim_ai_processor = dspy.Predict(AILanguageDetector)    
    optim_ai_processor.load("mipro-prompt.json")
    evaluate_prompt(optim_ai_processor,full_dspy_dataset)
